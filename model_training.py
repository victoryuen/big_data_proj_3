import time
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score

# X = inputs
# y = output
# so X_train: training input, y_train: training expected output
#    X_test: testing input, y_test: testing expected input
#    y_pred: output generated by model
#            how did it do compared to expected?


def train_model(data: dict[str, list], classifier: any, kernel: str = "") -> dict[str, any]:
    """
    From the given data, train a model of the given classifier.
    Returns the model itself and the time it took to train it
    """
    if kernel == "":
        clf = classifier()
    else:
        clf = classifier(kernel=kernel)

    start_time = time.time()
    clf.fit(data["X_train"], data["y_train"])
    elapsed_time = time.time() - start_time

    return {"model": clf, "time": elapsed_time}

def evaluate_model(data: dict[str, list], model: any) -> dict[str, any]:
    """
    Runs the given model on testing data, 
    and return evaluatory metrics,
    like accuracy, sensitivity, specificity, confusion matrix
    """
    y_pred = model.predict(data["X_test"])

    confusion = confusion_matrix(data["y_test"], y_pred)
    accuracy = accuracy_score(data["y_test"], y_pred)
    sensitivity = recall_score(data["y_test"], y_pred, pos_label="M")
    specificity = recall_score(data["y_test"], y_pred, pos_label="B")
    # specificity = sensitivity of the negative class
 
    return {"confusion matrix": confusion, "accuracy": accuracy, "sensitivity": sensitivity, "specificity": specificity}